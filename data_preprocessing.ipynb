{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xarray in /opt/conda/lib/python3.6/site-packages (0.11.3)\n",
      "Requirement already satisfied: pandas>=0.19.2 in /opt/conda/lib/python3.6/site-packages (from xarray) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.12 in /opt/conda/lib/python3.6/site-packages (from xarray) (1.13.3)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.19.2->xarray) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.19.2->xarray) (2.7.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas>=0.19.2->xarray) (1.12.0)\n",
      "Requirement already satisfied: bottleneck in /opt/conda/lib/python3.6/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from bottleneck) (1.13.3)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras) (1.13.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from keras) (1.0.9)\n",
      "Collecting git+https://github.com/pierre-rouanet/dtw\n",
      "  Cloning https://github.com/pierre-rouanet/dtw to /tmp/pip-req-build-87qkdgr6\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from dtw==1.3.3) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/conda/lib/python3.6/site-packages (from dtw==1.3.3) (1.1.0)\n",
      "Building wheels for collected packages: dtw\n",
      "  Building wheel for dtw (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-ih667cvu/wheels/a3/ce/63/d2baa04d18e71bac957ded215a61a756e362867fb49d3262a4\n",
      "Successfully built dtw\n",
      "Installing collected packages: dtw\n",
      "  Found existing installation: dtw 1.3.3\n",
      "    Uninstalling dtw-1.3.3:\n",
      "      Successfully uninstalled dtw-1.3.3\n",
      "Successfully installed dtw-1.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xarray\n",
    "!pip install bottleneck\n",
    "!pip install keras\n",
    "!pip install --upgrade git+https://github.com/pierre-rouanet/dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import io\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data download helper scripts\n",
    "\n",
    "Helper code for automating download of raw data directly from KNMI. Data must have been recently generated and published on KNMI server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7dfecc91f071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m65\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtasmin_uri_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://climexp.knmi.nl/data/icmip5_tasmin_Amon_ens_rcp26_-125--65E_25-50N_n_'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtasmax_uri_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://climexp.knmi.nl/data/icmip5_tasmax_Amon_ens_rcp26_-125--65E_25-50N_n_'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wget'"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "N_models = 65\n",
    "tasmin_uri_base = 'https://climexp.knmi.nl/data/icmip5_tasmin_Amon_ens_rcp26_-125--65E_25-50N_n_'\n",
    "tasmax_uri_base = 'https://climexp.knmi.nl/data/icmip5_tasmax_Amon_ens_rcp26_-125--65E_25-50N_n_'\n",
    "pr_uri_base = 'https://climexp.knmi.nl/data/icmip5_pr_Amon_ens_rcp26_-125--65E_25-50N_n_'\n",
    "evspspl_uri_base = 'https://climexp.knmi.nl/data/icmip5_evspsbl_Amon_ens_rcp26_-125--65E_25-50N_n_'\n",
    "pme_uri_base = 'https://climexp.knmi.nl/data/icmip5_pme_Amon_ens_rcp26_-125--65E_25-50N_n_'\n",
    "hurs_uri_base = 'https://climexp.knmi.nl/data/icmip5_hurs_Amon_ens_rcp26_-125--65E_25-50N_n_'\n",
    "psl_uri_base = 'https://climexp.knmi.nl/data/icmip5_psl_Amon_ens_rcp26_-125--65E_25-50N_n_'\n",
    "for i in range(63, N_models):\n",
    "    uri = '{}{:03d}.dat'.format(psl_uri_base, i)\n",
    "    print('Downloading model {}/{} from {}'.format(i+1, N_models, uri))\n",
    "    wget.download(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for raw data files\n",
    "\n",
    "1. Parse and save raw data into per variable datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = 'raw_data'\n",
    "var_names = ['tas','tasmin','tasmax','pr','pme','evspsbl']\n",
    "months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# create data/ directory\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "def parse_model_file(filename: str) -> pd.DataFrame:\n",
    "    def parse_header(lines: List[str]) -> Tuple[Dict[str, str], int]:\n",
    "        metadata = dict()\n",
    "        for (i, line) in enumerate(lines):\n",
    "            # stop at end of header\n",
    "            if not line.startswith('#'):\n",
    "                return metadata, i\n",
    "            # skip header lines that not in key-value format\n",
    "            if not '::' in line:\n",
    "                continue\n",
    "            kv = line.replace('#', '').split('::')\n",
    "            assert len(kv) == 2\n",
    "            metadata[kv[0].strip()] = kv[1].strip()\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        metadata, i = parse_header(lines)\n",
    "        csv_str = \"\".join(lines[i:])\n",
    "        df = pd.read_csv(io.StringIO(csv_str), delim_whitespace=True, header=None)\n",
    "        years = df[0]\n",
    "        df = df.drop(columns=[0])\n",
    "        name = '{}_{}'.format(metadata['model_id'], metadata['realization'])\n",
    "        xdarr = xr.DataArray(df, coords=[years, months], dims=['years', 'months'], attrs=metadata, name=name)\n",
    "        return xdarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in var_names:\n",
    "    dirname = './{}/{}'.format(raw_data_dir, var)\n",
    "    models = dict()\n",
    "    for file in filter(lambda f: not f.startswith('.'), os.listdir(dirname)):\n",
    "        xdarr = parse_model_file('{}/{}'.format(dirname, file))\n",
    "        models[xdarr.name] = xdarr\n",
    "    ds = xr.Dataset(models)\n",
    "    ds.to_netcdf(\"./data/{}.nc\".format(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Collect and organize raw data to construct per-model datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def center_monthly_means(xdarr: xr.DataArray) -> xr.DataArray:\n",
    "    attrs = xdarr.attrs\n",
    "    means = xdarr.mean(dim='years', keep_attrs=True)\n",
    "    xdarr = xdarr - means\n",
    "    xdarr.attrs = attrs\n",
    "    return xdarr\n",
    "\n",
    "def flatten_months(xdarr: xr.DataArray) -> xr.DataArray:\n",
    "    d0,d1 = xdarr.shape\n",
    "    xdarr = xdarr.stack(time=('years','months'))\n",
    "    assert(len(xdarr.shape) == 1)\n",
    "    assert(xdarr.shape[0] == d0*d1)\n",
    "    return xdarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing model data for tas\n",
      "building data array for tas\n",
      "processing model data for tasmin\n",
      "building data array for tasmin\n",
      "processing model data for tasmax\n",
      "building data array for tasmax\n",
      "processing model data for pr\n",
      "building data array for pr\n",
      "processing model data for pme\n",
      "building data array for pme\n",
      "processing model data for evspsbl\n",
      "building data array for evspsbl\n",
      "building dataset for all variables\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (models: 40, time: 2880)\n",
      "Coordinates:\n",
      "  * models   (models) <U16 'CCSM4_1' 'CCSM4_2' ... 'MRI-CGCM3_1' 'NorESM1-M_1'\n",
      "  * time     (time) MultiIndex\n",
      "  - years    (time) int64 1861 1861 1861 1861 1861 ... 1863 1863 1863 1863 1863\n",
      "  - months   (time) object 'Jan' 'Feb' 'Mar' 'Apr' ... 'Mar' 'Apr' 'May' 'Jun'\n",
      "Data variables:\n",
      "    tas      (models, time) float64 -1.232 -0.9981 -0.3392 ... 1.091 1.176\n",
      "    tasmin   (models, time) float64 -0.8222 -3.135 -0.5128 ... 1.486 0.5544\n",
      "    tasmax   (models, time) float64 -0.5729 -2.731 -0.5294 ... 1.633 -0.1004\n",
      "    pr       (models, time) float64 -5.454e-07 -3.203e-06 ... -1.802e-06\n",
      "    pme      (models, time) float64 -1.763e-07 -3.496e-06 ... -5.584e-06\n",
      "    evspsbl  (models, time) float64 -3.692e-07 2.936e-07 ... 3.288e-06 3.781e-06\n"
     ]
    }
   ],
   "source": [
    "var_datasets = dict()\n",
    "for var in var_names:\n",
    "    xds = xr.open_dataset('data/{}.nc'.format(var))\n",
    "    var_datasets[var] = xds\n",
    "    \n",
    "common_models = set()\n",
    "for var, ds in var_datasets.items():\n",
    "    if len(common_models) == 0:\n",
    "        common_models |= ds.data_vars.keys()\n",
    "    else:\n",
    "        common_models &= ds.data_vars.keys()\n",
    "var_data = dict()\n",
    "for var, ds in var_datasets.items():\n",
    "    xs = []\n",
    "    model_names = []\n",
    "    print('processing model data for {}'.format(var))\n",
    "    for model in sorted(filter(lambda m: m in common_models, ds.data_vars.keys())):\n",
    "        xdarr = ds.data_vars[model]\n",
    "        # fill NaNs\n",
    "        xdarr = xdarr.ffill(dim='years')\n",
    "        xdarr = xdarr.bfill(dim='years')\n",
    "        xdarr = center_monthly_means(xdarr)\n",
    "        xdarr = flatten_months(xdarr)\n",
    "        xs.append(xdarr)\n",
    "        model_names.append(model)\n",
    "    print('building data array for {}'.format(var))\n",
    "    var_dr = xr.DataArray(xs, [('models', model_names),('time', xs[0].indexes['time'])])\n",
    "    var_data[var] = var_dr\n",
    "\n",
    "print('building dataset for all variables')\n",
    "model_time_var_ds = xr.Dataset(var_data)\n",
    "print(model_time_var_ds)\n",
    "model_time_var_ds = model_time_var_ds.reset_index('time')\n",
    "print(model_time_var_ds)\n",
    "model_time_var_ds.to_netcdf('./data/{}.nc'.format('models_all_vars_vs_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import SpectralEmbedding, TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import dtw, accelerated_dtw\n",
    "from typing import Callable\n",
    "from multiprocessing import Pool\n",
    "from itertools import product\n",
    "\n",
    "def _pardtw(params):\n",
    "    x_i, x_j, metric = params\n",
    "    d,cost,acc_cost,path = accelerated_dtw(x_i, x_j, metric)\n",
    "    return d\n",
    "\n",
    "def pdtw(X, metric: str, verbose: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns a function d: X x X -> R that calculates DTW distances from\n",
    "    a tensor space X, where the second dim of X is time.\n",
    "    X : data matrix\n",
    "    metric : metric name to use for DTW (see scipy cdist)\n",
    "    \"\"\"\n",
    "    n, t, m = X.shape\n",
    "    pool = Pool(4)\n",
    "    results = pool.map(_pardtw, [(X[i], X[j], metric) for i in range(n) for j in range(n)])\n",
    "    return np.array(results).reshape((n,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray (models: 40, time: 2880, variable: 6)>\n",
      "array([[[ -1.232390e+00,  -8.221712e-01, ...,  -1.762584e-07,  -3.691904e-07],\n",
      "        [ -9.980754e-01,  -3.134700e+00, ...,  -3.496395e-06,   2.935993e-07],\n",
      "        ..., \n",
      "        [  1.093070e+00,   9.386808e-01, ...,  -3.006784e-06,   2.310148e-06],\n",
      "        [  1.049638e+00,   1.553081e+00, ...,   5.685991e-06,   2.558547e-06]],\n",
      "\n",
      "       [[ -6.226550e-01,  -2.455808e-01, ...,   1.469825e-06,   5.744220e-07],\n",
      "        [ -8.767850e-01,   5.197417e-01, ...,  -7.336508e-06,  -3.874350e-08],\n",
      "        ..., \n",
      "        [  9.840863e-01,   1.147870e+00, ...,  -1.667757e-06,   2.476872e-07],\n",
      "        [  3.382746e-01,   7.926292e-01, ...,  -5.932489e-06,   3.007603e-06]],\n",
      "\n",
      "       ..., \n",
      "       [[ -1.374901e+00,  -1.929253e+00, ...,   1.195963e-05,  -6.273888e-06],\n",
      "        [ -5.095788e-01,  -3.281883e+00, ...,   7.633096e-06,   1.095362e-06],\n",
      "        ..., \n",
      "        [  4.450033e-01,   8.567608e-01, ...,  -1.969318e-06,   2.737182e-06],\n",
      "        [  5.749437e-01,   1.901202e+00, ...,  -3.146661e-06,  -2.184425e-06]],\n",
      "\n",
      "       [[ -4.495746e-01,  -1.013100e+00, ...,   6.291128e-06,  -3.593820e-06],\n",
      "        [ -4.946579e-01,  -1.308242e-01, ...,   1.386026e-06,   2.252870e-06],\n",
      "        ..., \n",
      "        [  1.090548e+00,   1.485770e+00, ...,  -3.829500e-06,   3.288378e-06],\n",
      "        [  1.176182e+00,   5.543529e-01, ...,  -5.583992e-06,   3.781489e-06]]])\n",
      "Coordinates:\n",
      "    months    (time) object ...\n",
      "    years     (time) int32 ...\n",
      "  * models    (models) object 'CCSM4_1' 'CCSM4_2' ... 'NorESM1-M_1'\n",
      "  * variable  (variable) <U7 'tas' 'tasmin' 'tasmax' 'pr' 'pme' 'evspsbl'\n",
      "Dimensions without coordinates: time\n"
     ]
    }
   ],
   "source": [
    "ds = xr.open_dataset('data/models_all_vars_vs_time.nc')\n",
    "X_ds = ds.to_array().transpose('models', 'time', 'variable')\n",
    "print(X_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_x = pdtw(X_ds, 'euclidean')\n",
    "print(D_x.shape)\n",
    "print(D_x)\n",
    "np.save('/data/dtw.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
